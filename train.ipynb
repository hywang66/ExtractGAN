{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "from torch.nn import functional as F\n",
    "import torchvision.models\n",
    "from options.train_options import TrainOptions\n",
    "import sys; sys.argv=['']; del sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %tb \n",
    "opt = TrainOptions().parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.init_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaseModule, self).__init__()\n",
    "    \n",
    "    def weights_init_func(self, m, init_type, gain):\n",
    "        if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "            if init_type == 'normal':\n",
    "                init.normal_(m.weight.data, 0.0, gain)\n",
    "            elif init_type == 'xavier':\n",
    "                init.xavier_normal_(m.weight.data, gain=gain)\n",
    "            elif init_type == 'kaiming':\n",
    "                init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
    "            elif init_type == 'orthogonal':\n",
    "                init.orthogonal_(m.weight.data, gain=gain)\n",
    "            else:\n",
    "                raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n",
    "            if hasattr(m, 'bias') and m.bias is not None:\n",
    "                init.constant_(m.bias.data, 0.0)\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            init.normal_(m.weight.data, 1.0, gain)\n",
    "            init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "\n",
    "class Encoder(BaseModule):\n",
    "    def __init__(self, input_nc=3, same_size_nf=[64, 64], init_type='xavier', \n",
    "                 norm_layer=nn.InstanceNorm2d, padding_type='zero'):\n",
    "        super(Encoder, self).__init__()\n",
    "        padding_layer = nn.ReflectionPad2d if padding_type == 'reflect' else nn.ZeroPad2d\n",
    "        use_bias = norm_layer == nn.InstanceNorm2d\n",
    "        \n",
    "        model = []\n",
    "\n",
    "        # same-size layers\n",
    "        last_nf = input_nc\n",
    "        for nf in same_size_nf:\n",
    "            model += [\n",
    "                padding_layer(1),\n",
    "                nn.Conv2d(last_nf, nf, kernel_size=3, padding=0, bias=use_bias),\n",
    "                norm_layer(nf),\n",
    "                nn.ReLU(inplace=True),\n",
    "                ]\n",
    "            last_nf = nf\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "        \n",
    "        weights_init_func = lambda m : self.weights_init_func(m, init_type, gain=0.02)\n",
    "        self.apply(weights_init_func)\n",
    "        print('Encoder weights initialized using %s.' % init_type)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.model(input)\n",
    "\n",
    "\n",
    "class Decoder(BaseModule):\n",
    "    def __init__(self, output_nc=3, same_size_nf=[64, 64], init_type='xavier',\n",
    "                 norm_layer=nn.InstanceNorm2d, padding_type='zero'):\n",
    "        super(Decoder, self).__init__()\n",
    "        padding_layer = nn.ReflectionPad2d if padding_type == 'reflect' else nn.ZeroPad2d\n",
    "        use_bias = norm_layer == nn.InstanceNorm2d\n",
    "        \n",
    "        model = []\n",
    "\n",
    "        # same-size layers\n",
    "        for i, nf in enumerate(same_size_nf):\n",
    "            next_nf = output_nc if i == len(same_size_nf) - 1 else same_size_nf[i + 1]\n",
    "            model += [\n",
    "                padding_layer(1),\n",
    "                nn.ConvTranspose2d(nf, next_nf, kernel_size=3, padding=0, bias=use_bias),\n",
    "                norm_layer(nf),\n",
    "                nn.Tanh(),\n",
    "                ]\n",
    "\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "        weights_init_func = lambda m : self.weights_init_func(m, init_type, gain=0.02)\n",
    "        self.apply(weights_init_func)\n",
    "        print('Decoder weights initialized using %s.' % init_type)\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.model(input)\n",
    "\n",
    "\n",
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(self, dim=64, padding_type='zero', norm_layer = nn.InstanceNorm2d, use_dropout=False):\n",
    "        super(ResnetBlock, self).__init__()\n",
    "        use_bias = norm_layer == nn.InstanceNorm2d\n",
    "        padding_layer = nn.ReflectionPad2d if padding_type == 'reflect' else nn.ZeroPad2d\n",
    "        conv_block = []\n",
    "\n",
    "        conv_block += [\n",
    "            padding_layer(1),\n",
    "            nn.Conv2d(dim, dim, kernel_size=3, padding=0, bias=use_bias),\n",
    "            norm_layer(dim),\n",
    "            nn.ReLU(True),\n",
    "            ]\n",
    "\n",
    "        if use_dropout:\n",
    "            conv_block += [nn.Dropout(0.5)]\n",
    "\n",
    "        conv_block += [\n",
    "            padding_layer(1),\n",
    "            nn.Conv2d(dim, dim, kernel_size=3, padding=0, bias=use_bias),\n",
    "            norm_layer(dim),\n",
    "            ]\n",
    "\n",
    "        self.conv_block = nn.Sequential(*conv_block)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(x + self.conv_block(x), inplace=True)\n",
    "        return out\n",
    "  \n",
    "\n",
    "class StyleExtractor(BaseModule):\n",
    "    def __init__(self, n_kernel_channels=64, init_type='xavier', n_hidden=1024):\n",
    "        super(StyleExtractor, self).__init__()\n",
    "        # self.cfg = [[64], [64, 'M', 128], [128, 'M', 256], [256, 256, 'M', 512]]\n",
    "        self.cfg = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M']\n",
    "        # output size: 224*224*64, 112*112*128, 56*56*256, 28*28*512, 14*14*512, 7*7*512\n",
    "        \n",
    "        # self.vgg_block_set = set()\n",
    "        # self.vgg_block_name_template='vgg_block_%d'\n",
    "        # self.nblocks = len(self.cfg)\n",
    "        self.nkc = n_kernel_channels\n",
    "        self.vgg16 = None\n",
    "        self.make_partial_vgg16()\n",
    "        # self.n_vgg_parameters = len(list(self.vgg16.parameters()))\n",
    "        self.init_vgg16()\n",
    "    \n",
    "        self.representor = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, n_hidden),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            # nn.Linear(4096, 4096),\n",
    "            # nn.ReLU(True),\n",
    "            # nn.Dropout()\n",
    "        )\n",
    "        self.conv_kernel_gen_1 = nn.Linear(n_hidden, n_kernel_channels*n_kernel_channels*3*3)\n",
    "        self.conv_kernel_gen_2 = nn.Linear(n_hidden, n_kernel_channels*n_kernel_channels*3*3)\n",
    "        self.conv_kernel_gen_3 = nn.Linear(n_hidden, n_kernel_channels*n_kernel_channels*3*3)\n",
    "        self.conv_kernel_gen_4 = nn.Linear(n_hidden, n_kernel_channels*n_kernel_channels*3*3)\n",
    "        \n",
    "        weights_init_func = lambda m : self.weights_init_func(m, init_type, gain=0.02)\n",
    "        for module in self.children():\n",
    "            if module is not self.vgg16:\n",
    "                module.apply(weights_init_func)       \n",
    "        print('StyleExtractor weights initialized using %s.' % init_type)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        conv_kernels = []\n",
    "        features = self.vgg16(x).detach()\n",
    "        deep_features = self.representor(features)\n",
    "        conv_kernels.append(self.conv_kernel_gen_1(deep_features).view(self.nkc, self.nkc, 3, 3))\n",
    "        conv_kernels.append(self.conv_kernel_gen_2(deep_features).view(self.nkc, self.nkc, 3, 3))\n",
    "        conv_kernels.append(self.conv_kernel_gen_3(deep_features).view(self.nkc, self.nkc, 3, 3))\n",
    "        conv_kernels.append(self.conv_kernel_gen_4(deep_features).view(self.nkc, self.nkc, 3, 3))\n",
    "        # vgg_outputs = []\n",
    "        # src = x\n",
    "        # for i in range(self.nblocks):\n",
    "        #     block = getattr(self, self.vgg_block_name_template%i)\n",
    "        #     dst  = block(src)\n",
    "        #     vgg_outputs.append(dst)\n",
    "        #     src = dst\n",
    "        return conv_kernels\n",
    "\n",
    "    # def make_partial_vgg16(self):\n",
    "    #     features = []\n",
    "    #     in_channels = 3\n",
    "    #     for i, c in enumerate(self.cfg):\n",
    "    #         layers = []\n",
    "    #         for v in c:\n",
    "    #             if v == 'M':\n",
    "    #                 layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "    #             else:\n",
    "    #                 conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "    #                 layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "    #                 in_channels = v\n",
    "    #         block = nn.Sequential(*layers)\n",
    "    #         setattr(self, self.vgg_block_name_template%i, block)\n",
    "    #         self.vgg_block_set.add(block)\n",
    "\n",
    "    def make_partial_vgg16(self):\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        for v in self.cfg:\n",
    "            if v == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "                in_channels = v\n",
    "        self.vgg16 = nn.Sequential(*layers)\n",
    "                      \n",
    "    def train(self, mode=True):\n",
    "        r\"\"\"\n",
    "        Override the train method inherited from nn.Module to keep vgg blocks always in train mode.\n",
    "        \"\"\"\n",
    "        self.training = mode\n",
    "        for module in self.children():\n",
    "            # if module in self.vgg_block_set:\n",
    "            if module in self.vgg_layer_set:\n",
    "                module.train(False)\n",
    "            else:\n",
    "                module.train(mode)\n",
    "        return self\n",
    "    \n",
    "    def init_vgg16(self):\n",
    "        vgg16_state_dict = torchvision.models.vgg16_bn(pretrained=True).state_dict()\n",
    "        dict_new = self.state_dict().copy()\n",
    "        new_list = list(self.state_dict().keys())\n",
    "        trained_list = list(vgg16_state_dict.keys())\n",
    "        \n",
    "        # for i in range(self.n_vgg_parameters):\n",
    "        for i, _ in enumerate(self.vgg16.parameters()):\n",
    "            dict_new[new_list[i]] = vgg16_state_dict[trained_list[i]]\n",
    "        \n",
    "        self.load_state_dict(dict_new)\n",
    "        print('VGG parameters loaded.')\n",
    "            \n",
    "\n",
    "class StyleWhitener(BaseModule):\n",
    "    def __init__(self, n_blocks=2, dim=64, init_type='xavier', padding_type='zero', \n",
    "                 norm_layer = nn.InstanceNorm2d, use_dropout=False):\n",
    "        super(StyleWhitener, self).__init__()\n",
    "        model = []\n",
    "        for _ in range(n_blocks):\n",
    "            model.append(\n",
    "                ResnetBlock(dim=64, padding_type='zero', \n",
    "                norm_layer = nn.InstanceNorm2d, use_dropout=False)\n",
    "                )\n",
    "        self.model = nn.Sequential(*model)\n",
    "        \n",
    "        weights_init_func = lambda m : self.weights_init_func(m, init_type, gain=0.02)\n",
    "        self.apply(weights_init_func)\n",
    "        print('StyleWhitener weights initialized using %s.' % init_type)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    " \n",
    "\n",
    "class KernelSpecifiedResnetBlock(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(KernelSpecifiedResnetBlock, self).__init__()\n",
    "\n",
    "    def forward(self, x, kernel1, kernel2):\n",
    "        conv1 = F.conv2d(x, kernel1, padding=1)\n",
    "        conv1_relu = F.relu(conv1, inplace=True)\n",
    "        conv2 = F.conv2d(conv1_relu, kernel2, padding=1)\n",
    "        out = F.relu(x + conv2, inplace=True)\n",
    "        return out\n",
    "  \n",
    "\n",
    "\n",
    "class Stylizer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Stylizer, self).__init__()\n",
    "        self.ksr_block1 = KernelSpecifiedResnetBlock()\n",
    "        self.ksr_block2 = KernelSpecifiedResnetBlock()\n",
    "    \n",
    "    def forward(self, whitened, conv_kernels):\n",
    "        tmp = self.ksr_block1(whitened, conv_kernels[0], conv_kernels[1])\n",
    "        return self.ksr_block2(tmp, conv_kernels[3], conv_kernels[4])\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_nc=3, init_type='xavier', \n",
    "                 norm_layer=nn.InstanceNorm2d, padding_type='zero'):\n",
    "        super(Generator, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "        self.style_extractor = StyleExtractor()\n",
    "        self.style_whitener = StyleWhitener()\n",
    "        self.stylizer = Stylizer()\n",
    "        print('Generator build success!')\n",
    "\n",
    "    def forward(self, content_img, style_img):\n",
    "        encoded = self.encoder(content_img)\n",
    "        whitened = self.style_whitener(encoded)\n",
    "        conv_kernels = self.style_extractor(style_img)\n",
    "        stylized = self.stylizer(whitened, conv_kernels)\n",
    "        out = self.decoder(stylized)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder weights initialized using xavier.\n",
      "Decoder weights initialized using xavier.\n",
      "VGG parameters loaded.\n",
      "StyleExtractor weights initialized using xavier.\n",
      "StyleWhitener weights initialized using xavier.\n",
      "Generator build success!\n"
     ]
    }
   ],
   "source": [
    "tnet = Generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
